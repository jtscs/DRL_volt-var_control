{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tf_agents\n",
        "!pip install dm-reverb[tensorflow]\n",
        "!pip install pandapower\n",
        "!pip install numba==0.56.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tn1mlmdzQYg",
        "outputId": "814fbef9-f4b7-45db-f3fd-db536ee848cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf_agents\n",
            "  Downloading tf_agents-0.18.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf_agents)\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf_agents) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (4.5.0)\n",
            "Collecting pygame==2.1.3 (from tf_agents)\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-probability~=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf_agents) (0.22.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents) (0.0.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tf_agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tf_agents) (0.5.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tf_agents) (0.1.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697631 sha256=9ab13fda888978774f64997dd52a4774758677b452dabeae5b979351dc9d18cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
            "Successfully built gym\n",
            "Installing collected packages: pygame, gym, tf_agents\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.23.0 pygame-2.1.3 tf_agents-0.18.0\n",
            "Collecting dm-reverb[tensorflow]\n",
            "  Downloading dm_reverb-0.13.0-cp310-cp310-manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb[tensorflow]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb[tensorflow]) (1.5.2)\n",
            "Requirement already satisfied: tensorflow~=2.14.0 in /usr/local/lib/python3.10/dist-packages (from dm-reverb[tensorflow]) (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.14.0->dm-reverb[tensorflow]) (2.14.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb[tensorflow]) (5.9.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.14.0->dm-reverb[tensorflow]) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->dm-reverb[tensorflow]) (3.2.2)\n",
            "Installing collected packages: dm-reverb\n",
            "Successfully installed dm-reverb-0.13.0\n",
            "Collecting pandapower\n",
            "  Downloading pandapower-2.13.1.zip (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from pandapower) (1.5.3)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.10/dist-packages (from pandapower) (3.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pandapower) (1.11.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pandapower) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pandapower) (23.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pandapower) (4.66.1)\n",
            "Collecting deepdiff (from pandapower)\n",
            "  Downloading deepdiff-6.7.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->pandapower) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->pandapower) (2023.3.post1)\n",
            "Collecting ordered-set<4.2.0,>=4.0.2 (from deepdiff->pandapower)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0->pandapower) (1.16.0)\n",
            "Building wheels for collected packages: pandapower\n",
            "  Building wheel for pandapower (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandapower: filename=pandapower-2.13.1-py3-none-any.whl size=10803675 sha256=bab4f726371b738ffd676f61fee0f3d2ce2547ad3c1bb4ac2b6d24515b7d044d\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/58/fb/c5c16044f0046e905e2fe55de3af6c0a43bfcc3219ed5d3af8\n",
            "Successfully built pandapower\n",
            "Installing collected packages: ordered-set, deepdiff, pandapower\n",
            "Successfully installed deepdiff-6.7.1 ordered-set-4.1.0 pandapower-2.13.1\n",
            "Collecting numba==0.56.4\n",
            "  Downloading numba-0.56.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llvmlite<0.40,>=0.39.0dev0 (from numba==0.56.4)\n",
            "  Downloading llvmlite-0.39.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.24,>=1.18 in /usr/local/lib/python3.10/dist-packages (from numba==0.56.4) (1.23.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba==0.56.4) (67.7.2)\n",
            "Installing collected packages: llvmlite, numba\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.41.1\n",
            "    Uninstalling llvmlite-0.41.1:\n",
            "      Successfully uninstalled llvmlite-0.41.1\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.58.1\n",
            "    Uninstalling numba-0.58.1:\n",
            "      Successfully uninstalled numba-0.58.1\n",
            "Successfully installed llvmlite-0.39.1 numba-0.56.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import reverb\n",
        "import tempfile\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.ddpg import critic_network\n",
        "from tf_agents.agents.ddpg import actor_network\n",
        "from tf_agents.agents.sac import sac_agent\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.policies import greedy_policy\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.policies import PolicySaver\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.train import actor\n",
        "from tf_agents.train import learner\n",
        "from tf_agents.train import triggers\n",
        "from tf_agents.train.utils import spec_utils\n",
        "from tf_agents.train.utils import strategy_utils\n",
        "from tf_agents.train.utils import train_utils\n",
        "\n",
        "import shutil\n",
        "\n",
        "\n",
        "\n",
        "import abc\n",
        "#import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "\n",
        "import pandapower as pp\n",
        "import pandapower.networks as pn\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "\n",
        "import sqlite3\n",
        "from sqlite3 import Error\n",
        "\n",
        "import time\n",
        "\n",
        "from calendar import monthrange"
      ],
      "metadata": {
        "id": "LiVPvyB827PF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQLP4RSn3qRJ",
        "outputId": "1d709b6a-0095-41e9-e95e-c33335c5db01"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def get_number_of_days_in_month(month):\n",
        "  year = 2016\n",
        "  return monthrange(year, month)[1]\n",
        "\n",
        "\n",
        "#method to adjust profile length to different month length and comply with clock changes in march and october\n",
        "def get_month_profile_length(month):\n",
        "  month_profile_length = get_number_of_days_in_month(month) * 24 * 4\n",
        "  if month == 3 : month_profile_length -= 4\n",
        "  if month == 10: month_profile_length += 4\n",
        "  return month_profile_length\n",
        "\n",
        "\n",
        "def get_month_profile_start_in_list(month):\n",
        "  month_profile_start_counter = 0\n",
        "  for i in range(1,month):\n",
        "    month_profile_start_counter += get_month_profile_length(i)\n",
        "\n",
        "  return month_profile_start_counter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#class to implement cigre medium voltage distribution net with pv and wind (https://pandapower.readthedocs.io/en/v2.3.0/networks/cigre.html) from pandas power as a learning environment for reinforcement learning\n",
        "class CigrePPEnv(py_environment.PyEnvironment):\n",
        "\n",
        "  #environment object constructor;\n",
        "  #bool_log_env: true for logging, false to shut off logging\n",
        "  #log_file_name: name of logging file, can be any string if bool_log_env if false\n",
        "  #month_list: list for months that shall be used in training\n",
        "  #train_mode: 0=use all months in month list for one episode; 1=use one random month in month list for one episode\n",
        "  #name: string used for text output overview\n",
        "\n",
        "  def __init__(self, bool_log_env, log_file_name, month_list, train_mode, name):\n",
        "    #action are 9 shift angels of s_gen\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(9,), dtype=np.float32, minimum=0, maximum=1, name='action')\n",
        "    #observation is all 15 bus voltages and all 9 previous shift angles\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(24,), dtype=np.float32, minimum=0, maximum=1, name='observation')\n",
        "    #action and observation are normalized to values between 0 and 1. Usage of these values is adapted\n",
        "\n",
        "    self._log_env = bool_log_env\n",
        "    self._month_list = month_list\n",
        "    self._train_mode = train_mode\n",
        "    self._name = name\n",
        "\n",
        "    if self._train_mode < 0 or self._train_mode > 1:\n",
        "      print(\"train mode must be 0 or 1!\")\n",
        "      quit()\n",
        "\n",
        "\n",
        "    self._state = [0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5]\n",
        "    self._step_counter = 0\n",
        "    self._continuous_step_counter = 0\n",
        "    self._episode_ended = False\n",
        "    self._episode_average_error = 0\n",
        "\n",
        "    if self._train_mode == 0:\n",
        "     self._current_training_month = self._month_list[0]\n",
        "    elif self._train_mode == 1:\n",
        "      self._current_training_month = random.choice(self._month_list)\n",
        "\n",
        "    self._month_profile_length = get_month_profile_length(self._current_training_month)\n",
        "    self._current_profile_pointer = get_month_profile_start_in_list(self._current_training_month)\n",
        "    self._month_iterator = 0\n",
        "    self._mode0_month_step_counter = 0\n",
        "\n",
        "\n",
        "    load_df = pd.read_csv(\"/content/drive/MyDrive/DRL_volt-var_control/LoadProfile.csv\", sep=\";\")\n",
        "\n",
        "    self.load_time_list = load_df['time']\n",
        "    self.load1_p_list = load_df['mv_semiurb_pload']\n",
        "    self.load1_q_list = load_df['mv_semiurb_qload']\n",
        "    self.load2_p_list = load_df['mv_urban_pload']\n",
        "    self.load2_q_list = load_df['mv_urban_qload']\n",
        "    self.load3_p_list = load_df['mv_comm_pload']\n",
        "    self.load3_q_list = load_df['mv_comm_qload']\n",
        "    self.load4_p_list = load_df['lv_rural3_pload']\n",
        "    self.load4_q_list = load_df['lv_rural3_qload']\n",
        "    self.load5_p_list = load_df['lv_urban6_pload']\n",
        "    self.load5_q_list = load_df['lv_urban6_qload']\n",
        "    self.load6_p_list = load_df['mv_semiurb_pload']\n",
        "    self.load6_q_list = load_df['mv_semiurb_qload']\n",
        "    self.load7_p_list = load_df['mv_urban_pload']\n",
        "    self.load7_q_list = load_df['mv_urban_qload']\n",
        "    self.load9_p_list = load_df['mv_comm_pload']\n",
        "    self.load9_q_list = load_df['mv_comm_qload']\n",
        "\n",
        "    self.load11_p_list = load_df['lv_rural1_pload']\n",
        "    self.load11_q_list = load_df['lv_rural1_qload']\n",
        "    self.load12_p_list = load_df['lv_rural2_pload']\n",
        "    self.load12_q_list = load_df['lv_rural2_qload']\n",
        "    self.load13_p_list = load_df['lv_semiurb4_pload']\n",
        "    self.load13_q_list = load_df['lv_semiurb4_qload']\n",
        "    self.load14_p_list = load_df['lv_semiurb5_pload']\n",
        "    self.load14_q_list = load_df['lv_semiurb5_qload']\n",
        "\n",
        "    self.load16_p_list = load_df['mv_rural_pload']\n",
        "    self.load16_q_list = load_df['mv_rural_qload']\n",
        "    self.load17_p_list = load_df['lv_semiurb4_pload']\n",
        "    self.load17_q_list = load_df['lv_semiurb4_qload']\n",
        "\n",
        "\n",
        "    gen_df = pd.read_csv(\"/content/drive/MyDrive/DRL_volt-var_control/RESProfile.csv\", sep=\";\")\n",
        "\n",
        "    generation_multiplier = 1.5\n",
        "\n",
        "    self.res_time_list = gen_df['time']\n",
        "    self.res1_s_list = generation_multiplier*gen_df['PV8']\n",
        "    self.res2_s_list = generation_multiplier*gen_df['PV2']\n",
        "    self.res3_s_list = generation_multiplier*gen_df['PV5']\n",
        "    self.res4_s_list = generation_multiplier*gen_df['PV1']\n",
        "    self.res5_s_list = generation_multiplier*gen_df['PV6']\n",
        "    self.res6_s_list = generation_multiplier*gen_df['PV3']\n",
        "    self.res7_s_list = generation_multiplier*gen_df['PV4']\n",
        "    self.res8_s_list = generation_multiplier*gen_df['PV7']\n",
        "    self.res9_s_list = generation_multiplier*gen_df['WP8']\n",
        "\n",
        "\n",
        "\n",
        "    self.net = pp.create_empty_network()\n",
        "    self.init_net()\n",
        "\n",
        "    if self._log_env:\n",
        "      try:\n",
        "        self._connection = sqlite3.connect(log_file_name)\n",
        "        self._cursor = self._connection.cursor()\n",
        "      except Error as e:\n",
        "        print(e)\n",
        "\n",
        "      create_steps_table_query = \"\"\" CREATE TABLE IF NOT EXISTS steps (\n",
        "                                                  step integer PRIMARY KEY,\n",
        "                                                  time string,\n",
        "                                                  shift_angle_1 float,\n",
        "                                                  shift_angle_2 float,\n",
        "                                                  shift_angle_3 float,\n",
        "                                                  shift_angle_4 float,\n",
        "                                                  shift_angle_5 float,\n",
        "                                                  shift_angle_6 float,\n",
        "                                                  shift_angle_7 float,\n",
        "                                                  shift_angle_8 float,\n",
        "                                                  shift_angle_9 float,\n",
        "                                                  average_vm_pu_deviation float\n",
        "                                              ); \"\"\"\n",
        "\n",
        "      self.create_log_table(create_steps_table_query)\n",
        "\n",
        "      self.episodeStartTime = time.perf_counter()\n",
        "\n",
        "\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "\n",
        "\n",
        "  #method to reset environment to default values\n",
        "  def _reset(self):\n",
        "    self._state = [0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5]\n",
        "    self._step_counter = 0\n",
        "    self._episode_average_error = 0\n",
        "    self._episode_ended = False\n",
        "    self._mode0_month_step_counter = 0\n",
        "\n",
        "\n",
        "    if self._train_mode == 0:\n",
        "      self._current_training_month = self._month_list[0]\n",
        "      print(f\"current {self._name} month {self._current_training_month}\")\n",
        "    elif self._train_mode == 1:\n",
        "      self._current_training_month = random.choice(self._month_list)\n",
        "      print(f\"current {self._name} month {self._current_training_month}\")\n",
        "\n",
        "    self._month_profile_length = get_month_profile_length(self._current_training_month)\n",
        "    self._current_profile_pointer = get_month_profile_start_in_list(self._current_training_month)\n",
        "    self._month_iterator = 0\n",
        "\n",
        "    self.episodeStartTime = time.perf_counter()\n",
        "    return ts.restart(np.array(self._state, dtype=np.float32))\n",
        "\n",
        "\n",
        "\n",
        "  #method to do oe step in environment with one action and get one observation\n",
        "  def _step(self, action):\n",
        "\n",
        "    if self._episode_ended:\n",
        "      # The last action ended the episode. Ignore the current action and start a new episode.\n",
        "      return self.reset()\n",
        "\n",
        "    #database logging made issues witout float conversion, converted floats might deliver slightly different results\n",
        "    action_casted = [float(action[0]), float(action[1]), float(action[2]), float(action[3]), float(action[4]), float(action[5]), float(action[6]), float(action[7]), float(action[8])]\n",
        "\n",
        "\n",
        "    # update state and vary load for current step\n",
        "    self.update_loads(self._current_profile_pointer)\n",
        "    self.update_RES(self._current_profile_pointer)\n",
        "\n",
        "    #check if load_time_list and res_time_list are the same, only needed if profiles are changed to make sure they work together\n",
        "    #if(self.load_time_list[self._current_profile_pointer] != self.res_time_list[self._current_profile_pointer]):\n",
        "    #  print(\"load and res lists had a mismatch, quitting Training\")\n",
        "    #  quit()\n",
        "\n",
        "\n",
        "    current_vm_pu_list = self.createAndRunNetSimulation(action_casted)\n",
        "\n",
        "    current_vm_pu_list = current_vm_pu_list.astype(np.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #skip first element of vm_pu list since it is te bus connected to ext_grid whic has const voltage\n",
        "    current_average_vm_pu_deviation = 0\n",
        "    for vm_pu in current_vm_pu_list[1:]:\n",
        "      current_average_vm_pu_deviation += abs(1.0-vm_pu)\n",
        "    current_average_vm_pu_deviation /= len(current_vm_pu_list[1:])\n",
        "\n",
        "    #normalize current_vm_pu_list to values between 0 and 1\n",
        "    for i in range(len(current_vm_pu_list)):\n",
        "        current_vm_pu_list[i] /= 2\n",
        "\n",
        "\n",
        "    self._state = [*current_vm_pu_list , *action_casted]\n",
        "    #print(f\"state {self._state}\")\n",
        "\n",
        "\n",
        "    if self._log_env:\n",
        "      log_query = ''' INSERT INTO steps(step,time, shift_angle_1, shift_angle_2, shift_angle_3, shift_angle_4, shift_angle_5, shift_angle_6, shift_angle_7, shift_angle_8, shift_angle_9, average_vm_pu_deviation)\n",
        "                    VALUES(?,?,?,?,?,?,?,?,?,?,?,?) '''\n",
        "\n",
        "      self.log_action(log_query, [self._continuous_step_counter, self.load_time_list[self._current_profile_pointer], action_casted[0], action_casted[1], action_casted[2], action_casted[3], action_casted[4], action_casted[5], action_casted[6], action_casted[7], action_casted[8], current_average_vm_pu_deviation])\n",
        "\n",
        "\n",
        "    self._step_counter += 1\n",
        "    self._continuous_step_counter += 1\n",
        "    self._current_profile_pointer += 1\n",
        "    self._mode0_month_step_counter += 1\n",
        "\n",
        "    self._episode_average_error += current_average_vm_pu_deviation\n",
        "\n",
        "    #multipy reward with 100 because vm_pu_deviation is very small, since vm_pu_deviation is to be minimized it must be converted tobe negative because the reward is maximized\n",
        "    reward = -100 * current_average_vm_pu_deviation\n",
        "    #print(f\"reward {reward}\")\n",
        "\n",
        "\n",
        "    if(self._mode0_month_step_counter == (self._month_profile_length/2) or self._mode0_month_step_counter == self._month_profile_length):\n",
        "      current_average_error_per_step = self._episode_average_error/self._step_counter\n",
        "      print(f\"Current average Error per step in episode {current_average_error_per_step}\")\n",
        "\n",
        "\n",
        "\n",
        "    if(self._train_mode == 0):\n",
        "      if(self._mode0_month_step_counter >= self._month_profile_length):\n",
        "        #if the iterator was at last month of month_list the iteration over all month is complete and the episode must end in mode 0\n",
        "        if(self._month_iterator + 1 == len(self._month_list)):\n",
        "           self._episode_ended = True\n",
        "        else :\n",
        "          self._month_iterator += 1\n",
        "          self._current_training_month = self._month_list[self._month_iterator]\n",
        "          print(f\"current {self._name} month {self._current_training_month}\")\n",
        "          self._month_profile_length = get_month_profile_length(self._current_training_month)\n",
        "          self._current_profile_pointer = get_month_profile_start_in_list(self._current_training_month)\n",
        "          self._mode0_month_step_counter = 0\n",
        "\n",
        "\n",
        "    elif(self._train_mode == 1):\n",
        "      if(self._step_counter >= self._month_profile_length):\n",
        "        self._episode_ended = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if self._episode_ended:\n",
        "      if self._log_env:\n",
        "        self.sql_commit()\n",
        "\n",
        "      #print _episode_average_error to check how training is going\n",
        "      self._episode_average_error /= self._step_counter\n",
        "      print(f\"Average Error per step in episode {self._episode_average_error}\")\n",
        "      episodeEndTime = time.perf_counter()\n",
        "      print(f\"Epsidoe Time: {episodeEndTime - self.episodeStartTime:0.4f} seconds\")\n",
        "\n",
        "      return ts.termination(np.array(self._state, dtype=np.float32), reward)\n",
        "    else:\n",
        "      return ts.transition(np.array(self._state, dtype=np.float32), reward=reward, discount=0.0)\n",
        "\n",
        "\n",
        "\n",
        "  def init_net(self):\n",
        "    self.net = pn.create_cigre_network_mv(with_der=\"pv_wind\")\n",
        "    self.net.load.p_mw[0] = 0\n",
        "    self.net.load.q_mvar[0] = 0\n",
        "    self.net.load.p_mw[8] = 0\n",
        "    self.net.load.q_mvar[8] = 0\n",
        "    self.net.load.p_mw[10] = 0\n",
        "    self.net.load.q_mvar[10] = 0\n",
        "    self.net.load.p_mw[15] = 0\n",
        "    self.net.load.q_mvar[15] = 0\n",
        "\n",
        "\n",
        "  def is_episode_finished(self):\n",
        "    return self._episode_ended\n",
        "\n",
        "\n",
        "  def update_loads(self, step_counter):\n",
        "\n",
        "    self.net.load.p_mw[1] = self.load1_p_list[step_counter]\n",
        "    self.net.load.q_mvar[1] = self.load1_q_list[step_counter]\n",
        "    self.net.load.p_mw[2] = self.load2_p_list[step_counter]\n",
        "    self.net.load.q_mvar[2] = self.load2_q_list[step_counter]\n",
        "    self.net.load.p_mw[3] = self.load3_p_list[step_counter]\n",
        "    self.net.load.q_mvar[3] = self.load3_q_list[step_counter]\n",
        "    self.net.load.p_mw[4] = self.load4_p_list[step_counter]\n",
        "    self.net.load.q_mvar[4] = self.load4_q_list[step_counter]\n",
        "    self.net.load.p_mw[5] = self.load5_p_list[step_counter]\n",
        "    self.net.load.q_mvar[5] = self.load5_q_list[step_counter]\n",
        "    self.net.load.p_mw[6] = self.load6_p_list[step_counter]\n",
        "    self.net.load.q_mvar[6] = self.load6_q_list[step_counter]\n",
        "    self.net.load.p_mw[7] = self.load7_p_list[step_counter]\n",
        "    self.net.load.q_mvar[7] = self.load7_q_list[step_counter]\n",
        "    self.net.load.p_mw[9] = self.load9_p_list[step_counter]\n",
        "    self.net.load.q_mvar[9] = self.load9_q_list[step_counter]\n",
        "    self.net.load.p_mw[11] = self.load11_p_list[step_counter]\n",
        "    self.net.load.q_mvar[11] = self.load11_q_list[step_counter]\n",
        "    self.net.load.p_mw[12] = self.load12_p_list[step_counter]\n",
        "    self.net.load.q_mvar[12] = self.load12_q_list[step_counter]\n",
        "    self.net.load.p_mw[13] = self.load13_p_list[step_counter]\n",
        "    self.net.load.q_mvar[13] = self.load13_q_list[step_counter]\n",
        "    self.net.load.p_mw[14] = self.load14_p_list[step_counter]\n",
        "    self.net.load.q_mvar[14] = self.load14_q_list[step_counter]\n",
        "    self.net.load.p_mw[16] = self.load16_p_list[step_counter]\n",
        "    self.net.load.q_mvar[16] = self.load16_q_list[step_counter]\n",
        "    self.net.load.p_mw[17] = self.load17_p_list[step_counter]\n",
        "    self.net.load.q_mvar[17] = self.load17_q_list[step_counter]\n",
        "\n",
        "    #print(self.net.load)\n",
        "\n",
        "\n",
        "\n",
        "  def update_RES(self, step_counter):\n",
        "    self.net.sgen['sn_mva'][0] = self.res1_s_list[step_counter]\n",
        "    self.net.sgen['sn_mva'][1] = self.res2_s_list[step_counter]\n",
        "    self.net.sgen['sn_mva'][2] = self.res3_s_list[step_counter]\n",
        "    self.net.sgen['sn_mva'][3] = self.res4_s_list[step_counter]\n",
        "    self.net.sgen['sn_mva'][4] = self.res5_s_list[step_counter]\n",
        "    self.net.sgen['sn_mva'][5] = self.res6_s_list[step_counter]\n",
        "    self.net.sgen['sn_mva'][6] = self.res7_s_list[step_counter]\n",
        "    self.net.sgen['sn_mva'][7] = self.res8_s_list[step_counter]\n",
        "    self.net.sgen['sn_mva'][8] = self.res9_s_list[step_counter]\n",
        "\n",
        "    #print(self.net.sgen)\n",
        "\n",
        "\n",
        "\n",
        "  def createAndRunNetSimulation(self, action):\n",
        "\n",
        "    #print(f\"action {action}\")\n",
        "    #print(f\"pre sgens {self.net.sgen}\")\n",
        "\n",
        "    generator_counter = 0\n",
        "    for generator in self.net.sgen.name:\n",
        "      #print(f\"generator {generator}\")\n",
        "\n",
        "      current_phase_shift_rad = math.radians(action[generator_counter]*51.68-25.84)\n",
        "      self.net.sgen['p_mw'][generator_counter] = math.cos(current_phase_shift_rad) * self.net.sgen['sn_mva'][generator_counter]\n",
        "      self.net.sgen['q_mvar'][generator_counter] = math.sin(current_phase_shift_rad) * self.net.sgen['sn_mva'][generator_counter]\n",
        "      generator_counter += 1\n",
        "\n",
        "    #print(f\"post sgens {self.net.sgen}\")\n",
        "\n",
        "    try:\n",
        "      pp.runpp(self.net)\n",
        "    except:\n",
        "      print(\"could not calculate power flow\")\n",
        "      return 0\n",
        "\n",
        "    #print(net.res_trafo)\n",
        "    #print(net.res_line)\n",
        "    #print(net.res_load)\n",
        "    #print(net.res_ext_grid)\n",
        "    #print(f\"result {self.net.res_bus['vm_pu']}\")\n",
        "\n",
        "    return (self.net.res_bus['vm_pu'])\n",
        "\n",
        "\n",
        "\n",
        "  def create_log_table(self, create_table_sql):\n",
        "    try:\n",
        "        self._cursor.execute(create_table_sql)\n",
        "    except Error as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "\n",
        "  def log_action(self, log_query, log_data):\n",
        "    try:\n",
        "        self._cursor.execute(log_query, log_data)\n",
        "        #self._connection.commit()\n",
        "    except Error as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "\n",
        "  def sql_commit(self):\n",
        "    try:\n",
        "        self._connection.commit()\n",
        "    except Error as e:\n",
        "        print(e)\n"
      ],
      "metadata": {
        "id": "I_fPJFKz21vV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%rm -rf /content/tempdir\n",
        "os.mkdir(\"/content/tempdir\")"
      ],
      "metadata": {
        "id": "sGe3Oi5Z-IvR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LpGX7jPYxQRq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8bf77dc5-a2fb-4a1a-98a5-47fe304c7431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current collect month 1\n",
            "Initial Collect Actor\n",
            "Current average Error per step in episode 0.018151969653189475\n",
            "Current average Error per step in episode 0.018271633030449024\n",
            "Average Error per step in episode 0.018271633030449024\n",
            "Epsidoe Time: 145.5384 seconds\n",
            "current collect month 11\n",
            "Current average Error per step in episode 0.018039634865191254\n",
            "Current average Error per step in episode 0.01806468192990574\n",
            "Average Error per step in episode 0.01806468192990574\n",
            "Epsidoe Time: 135.2733 seconds\n",
            "current collect month 7\n",
            "Current average Error per step in episode 0.021722852537113778\n",
            "Current average Error per step in episode 0.0224166814976024\n",
            "Average Error per step in episode 0.0224166814976024\n",
            "Epsidoe Time: 139.6501 seconds\n",
            "current collect month 5\n",
            "Current average Error per step in episode 0.021568605453119345\n",
            "Current average Error per step in episode 0.022944085767775933\n",
            "Average Error per step in episode 0.022944085767775933\n",
            "Epsidoe Time: 140.5183 seconds\n",
            "current collect month 11\n",
            "Current average Error per step in episode 0.018049626841786362\n",
            "Current average Error per step in episode 0.018064615924266152\n",
            "Average Error per step in episode 0.018064615924266152\n",
            "Epsidoe Time: 134.7615 seconds\n",
            "current collect month 5\n",
            "Current average Error per step in episode 0.021556656635988703\n",
            "Current average Error per step in episode 0.02296241063384284\n",
            "Average Error per step in episode 0.02296241063384284\n",
            "Epsidoe Time: 141.4776 seconds\n",
            "current collect month 3\n",
            "Current average Error per step in episode 0.0174208980922492\n",
            "Current average Error per step in episode 0.018279706181482388\n",
            "Average Error per step in episode 0.018279706181482388\n",
            "Epsidoe Time: 139.3278 seconds\n",
            "current collect month 9\n",
            "Current average Error per step in episode 0.02056243009569628\n",
            "Current average Error per step in episode 0.019843530685951377\n",
            "Average Error per step in episode 0.019843530685951377\n",
            "Epsidoe Time: 135.0984 seconds\n",
            "current collect month 3\n",
            "Current average Error per step in episode 0.017412135593479972\n",
            "Current average Error per step in episode 0.01824605695715647\n",
            "Average Error per step in episode 0.01824605695715647\n",
            "Epsidoe Time: 141.7169 seconds\n",
            "current collect month 1\n",
            "Current average Error per step in episode 0.018129215170893994\n",
            "Current average Error per step in episode 0.018286197813951667\n",
            "Average Error per step in episode 0.018286197813951667\n",
            "Epsidoe Time: 138.3309 seconds\n",
            "current collect month 1\n",
            "Current average Error per step in episode 0.01809252053499219\n",
            "Current average Error per step in episode 0.018252612802610542\n",
            "Average Error per step in episode 0.018252612802610542\n",
            "Epsidoe Time: 139.8923 seconds\n",
            "current collect month 9\n",
            "Current average Error per step in episode 0.020566753413350864\n",
            "Current average Error per step in episode 0.019702649190430632\n",
            "Average Error per step in episode 0.019702649190430632\n",
            "Epsidoe Time: 134.2688 seconds\n",
            "current collect month 5\n",
            "Current average Error per step in episode 0.021375361646878935\n",
            "Current average Error per step in episode 0.02282740168571015\n",
            "Average Error per step in episode 0.02282740168571015\n",
            "Epsidoe Time: 136.9418 seconds\n",
            "current collect month 5\n",
            "Current average Error per step in episode 0.02150122045085818\n",
            "Current average Error per step in episode 0.022878329435538\n",
            "Average Error per step in episode 0.022878329435538\n",
            "Epsidoe Time: 137.8784 seconds\n",
            "current collect month 7\n",
            "Current average Error per step in episode 0.02145330380139079\n",
            "Current average Error per step in episode 0.022459561164031683\n",
            "Average Error per step in episode 0.022459561164031683\n",
            "Epsidoe Time: 138.8200 seconds\n",
            "current collect month 1\n",
            "Current average Error per step in episode 0.01819956433399942\n",
            "Current average Error per step in episode 0.01827539314591711\n",
            "Average Error per step in episode 0.01827539314591711\n",
            "Epsidoe Time: 137.3055 seconds\n",
            "current collect month 5\n",
            "Current average Error per step in episode 0.021412721129789487\n",
            "Initial Collect Actor finished\n",
            "current collect month 9\n",
            "current evaluation month 2\n",
            "Current average Error per step in episode 0.017270321754374717\n",
            "Current average Error per step in episode 0.017005573135413784\n",
            "current evaluation month 4\n",
            "Current average Error per step in episode 0.017923954700914017\n",
            "Current average Error per step in episode 0.01869228168272673\n",
            "current evaluation month 6\n",
            "Current average Error per step in episode 0.01906727768356115\n",
            "Current average Error per step in episode 0.01946239687956822\n",
            "current evaluation month 8\n",
            "Current average Error per step in episode 0.02013615126417899\n",
            "Current average Error per step in episode 0.020257274698214606\n",
            "current evaluation month 10\n",
            "Current average Error per step in episode 0.020337722158999986\n",
            "Current average Error per step in episode 0.020173106201763713\n",
            "current evaluation month 12\n",
            "Current average Error per step in episode 0.020076636420310136\n",
            "Current average Error per step in episode 0.019974444439004282\n",
            "Average Error per step in episode 0.019974444439004282\n",
            "Epsidoe Time: 855.2373 seconds\n",
            "current evaluation month 2\n",
            "Initial max return set to -34907.33984375\n",
            "Training\n",
            "Current average Error per step in episode 0.016156907811287848\n",
            "Current average Error per step in episode 0.015885980309001957\n",
            "Average Error per step in episode 0.015885980309001957\n",
            "Epsidoe Time: 1306.6781 seconds\n",
            "step = 2880: loss = 0.2573609897659885\n",
            "average_actor_loss = 1.578646370919887 : average_critic_loss = 0.6220134983356628 : average_alpha_loss=-1.9432988790068824\n",
            "\n",
            "current collect month 7\n",
            "Current average Error per step in episode 0.015476174060306791\n",
            "Current average Error per step in episode 0.015569560219764076\n",
            "Average Error per step in episode 0.015569560219764076\n",
            "Epsidoe Time: 466.0147 seconds\n",
            "step = 5856: loss = -3.9203098104285297\n",
            "average_actor_loss = 1.586121573964114 : average_critic_loss = 0.38983208533897196 : average_alpha_loss=-5.896263476180774\n",
            "\n",
            "eval episode at step 5856\n",
            "Current average Error per step in episode 0.01726274674357651\n",
            "Current average Error per step in episode 0.01739870313683461\n",
            "current evaluation month 4\n",
            "Current average Error per step in episode 0.016618123494725676\n",
            "Current average Error per step in episode 0.016303400754856444\n",
            "current evaluation month 6\n",
            "Current average Error per step in episode 0.015969696859835156\n",
            "Current average Error per step in episode 0.015883030373365945\n",
            "current evaluation month 8\n",
            "Current average Error per step in episode 0.01583995633922214\n",
            "Current average Error per step in episode 0.015783381367856272\n",
            "current evaluation month 10\n",
            "Current average Error per step in episode 0.015816909559107944\n",
            "Current average Error per step in episode 0.01582396224273256\n",
            "current evaluation month 12\n",
            "Current average Error per step in episode 0.016019606785782573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
            "WARNING:absl:`0/reward` is not a valid tf.function parameter name. Sanitizing to `arg_0_reward`.\n",
            "WARNING:absl:`0/discount` is not a valid tf.function parameter name. Sanitizing to `arg_0_discount`.\n",
            "WARNING:absl:`0/observation` is not a valid tf.function parameter name. Sanitizing to `arg_0_observation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current average Error per step in episode 0.016123424242592818\n",
            "Average Error per step in episode 0.016123424242592818\n",
            "Epsidoe Time: 1773.4354 seconds\n",
            "current evaluation month 2\n",
            "step = 5856: AverageReturn = -28177.296875, AverageEpisodeLength = 17476.000000\n",
            "New best eval, saving policy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current collect month 5\n",
            "Current average Error per step in episode 0.015698563959878717\n",
            "Current average Error per step in episode 0.01535434468710843\n",
            "Average Error per step in episode 0.01535434468710843\n",
            "Epsidoe Time: 472.2180 seconds\n",
            "step = 8832: loss = -8.011952195276496\n",
            "average_actor_loss = 1.5612911246716976 : average_critic_loss = 0.3408553214763762 : average_alpha_loss=-9.914098636117032\n",
            "\n",
            "current collect month 11\n",
            "Current average Error per step in episode 0.016261676719619166\n",
            "Current average Error per step in episode 0.016978240561568072\n",
            "Average Error per step in episode 0.016978240561568072\n",
            "Epsidoe Time: 458.4623 seconds\n",
            "step = 11712: loss = -11.9934986088011\n",
            "average_actor_loss = 1.5620917854209742 : average_critic_loss = 0.31045282936861945 : average_alpha_loss=-13.866043213009835\n",
            "\n",
            "eval episode at step 11712\n",
            "Current average Error per step in episode 0.017257635568823766\n",
            "Current average Error per step in episode 0.017394922404551563\n",
            "current evaluation month 4\n",
            "Current average Error per step in episode 0.01661396960453719\n",
            "Current average Error per step in episode 0.01630031219454328\n",
            "current evaluation month 6\n",
            "Current average Error per step in episode 0.01596691288974528\n",
            "Current average Error per step in episode 0.015880634710730833\n",
            "current evaluation month 8\n",
            "Current average Error per step in episode 0.015838373612844146\n",
            "Current average Error per step in episode 0.015781965077899012\n",
            "current evaluation month 10\n",
            "Current average Error per step in episode 0.01581582577459974\n",
            "Current average Error per step in episode 0.015822677168059237\n",
            "current evaluation month 12\n",
            "Current average Error per step in episode 0.01601816831857158\n",
            "Current average Error per step in episode 0.016122026411791557\n",
            "Average Error per step in episode 0.016122026411791557\n",
            "Epsidoe Time: 1796.5091 seconds\n",
            "current evaluation month 2\n",
            "step = 11712: AverageReturn = -28174.853516, AverageEpisodeLength = 17476.000000\n",
            "New best eval, saving policy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current collect month 5\n",
            "Current average Error per step in episode 0.01569790817717071\n",
            "Current average Error per step in episode 0.015353998092050344\n",
            "Average Error per step in episode 0.015353998092050344\n",
            "Epsidoe Time: 478.2736 seconds\n",
            "step = 14688: loss = -15.977908980461859\n",
            "average_actor_loss = 1.550817130874562 : average_critic_loss = 0.2892023042715605 : average_alpha_loss=-17.81792843309782\n",
            "\n",
            "current collect month 9\n",
            "Current average Error per step in episode 0.015402337409082893\n",
            "Current average Error per step in episode 0.015359556744436917\n",
            "Average Error per step in episode 0.015359556744436917\n",
            "Epsidoe Time: 463.2280 seconds\n",
            "step = 17568: loss = -19.969615166054833\n",
            "average_actor_loss = 1.54093014680677 : average_critic_loss = 0.2592683702862511 : average_alpha_loss=-21.769813656806946\n",
            "\n",
            "eval episode at step 17568\n",
            "Current average Error per step in episode 0.0172576692432459\n",
            "Current average Error per step in episode 0.01739495083512294\n",
            "current evaluation month 4\n",
            "Current average Error per step in episode 0.01661400281364557\n",
            "Current average Error per step in episode 0.01630033942915145\n",
            "current evaluation month 6\n",
            "Current average Error per step in episode 0.01596693596664815\n",
            "Current average Error per step in episode 0.01588065550030231\n",
            "current evaluation month 8\n",
            "Current average Error per step in episode 0.015838391090473483\n",
            "Current average Error per step in episode 0.015781982829942105\n",
            "current evaluation month 10\n",
            "Current average Error per step in episode 0.01581584108089186\n",
            "Current average Error per step in episode 0.01582269314738916\n",
            "current evaluation month 12\n",
            "Current average Error per step in episode 0.016018184095566778\n",
            "Current average Error per step in episode 0.01612204148665266\n",
            "Average Error per step in episode 0.01612204148665266\n",
            "Epsidoe Time: 1798.9516 seconds\n",
            "current evaluation month 2\n",
            "step = 17568: AverageReturn = -28174.878906, AverageEpisodeLength = 17476.000000\n",
            "current collect month 5\n",
            "Current average Error per step in episode 0.015709777412715804\n",
            "Current average Error per step in episode 0.015383083609357686\n",
            "Average Error per step in episode 0.015383083609357686\n",
            "Epsidoe Time: 478.1151 seconds\n",
            "step = 20544: loss = -23.93740390898079\n",
            "average_actor_loss = 1.5557917235519296 : average_critic_loss = 0.22850324043263032 : average_alpha_loss=-25.721698879554708\n",
            "\n",
            "current collect month 9\n",
            "Current average Error per step in episode 0.015395450077596181\n",
            "Current average Error per step in episode 0.015349890919964028\n",
            "Average Error per step in episode 0.015349890919964028\n",
            "Epsidoe Time: 461.3365 seconds\n",
            "step = 23424: loss = -27.919877923197216\n",
            "average_actor_loss = 1.552180899762445 : average_critic_loss = 0.2015252804228415 : average_alpha_loss=-29.67358410888248\n",
            "\n",
            "eval episode at step 23424\n",
            "Current average Error per step in episode 0.017216071636341097\n",
            "Current average Error per step in episode 0.017362114095567985\n",
            "current evaluation month 4\n",
            "Current average Error per step in episode 0.01657922131420636\n",
            "Current average Error per step in episode 0.01627950130979124\n",
            "current evaluation month 6\n",
            "Current average Error per step in episode 0.015952768663071527\n",
            "Current average Error per step in episode 0.015869605782506083\n",
            "current evaluation month 8\n",
            "Current average Error per step in episode 0.015834989170056886\n",
            "Current average Error per step in episode 0.01578083563640363\n",
            "current evaluation month 10\n",
            "Current average Error per step in episode 0.01581600295884741\n",
            "Current average Error per step in episode 0.015820758602302098\n",
            "current evaluation month 12\n",
            "Current average Error per step in episode 0.016015394429788332\n",
            "Current average Error per step in episode 0.01611916819337706\n",
            "Average Error per step in episode 0.01611916819337706\n",
            "Epsidoe Time: 1807.1013 seconds\n",
            "current evaluation month 2\n",
            "step = 23424: AverageReturn = -28169.857422, AverageEpisodeLength = 17476.000000\n",
            "New best eval, saving policy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current collect month 5\n",
            "Current average Error per step in episode 0.015712947360084002\n",
            "Current average Error per step in episode 0.015376254446953334\n",
            "Average Error per step in episode 0.015376254446953334\n",
            "Epsidoe Time: 480.1110 seconds\n",
            "step = 26400: loss = -31.90063913278682\n",
            "average_actor_loss = 1.5381219846506913 : average_critic_loss = 0.1867082222184587 : average_alpha_loss=-33.6254693295366\n",
            "\n",
            "current collect month 3\n",
            "Current average Error per step in episode 0.017274582326996673\n",
            "Current average Error per step in episode 0.01648117573937184\n",
            "Average Error per step in episode 0.01648117573937184\n",
            "Epsidoe Time: 479.5779 seconds\n",
            "step = 29372: loss = -35.915865067678375\n",
            "average_actor_loss = 1.5463574763743424 : average_critic_loss = 0.17985027599302628 : average_alpha_loss=-37.64207282008586\n",
            "\n",
            "eval episode at step 29372\n",
            "Current average Error per step in episode 0.017203064384596522\n",
            "Current average Error per step in episode 0.017354385970138976\n",
            "current evaluation month 4\n",
            "Current average Error per step in episode 0.01658160766501998\n",
            "Current average Error per step in episode 0.016273004236639396\n",
            "current evaluation month 6\n",
            "Current average Error per step in episode 0.015944080084485626\n",
            "Current average Error per step in episode 0.015860211722046498\n",
            "current evaluation month 8\n",
            "Current average Error per step in episode 0.01582087724289022\n",
            "Current average Error per step in episode 0.01576592560473178\n",
            "current evaluation month 10\n",
            "Current average Error per step in episode 0.01580129152492535\n",
            "Current average Error per step in episode 0.015808610597267462\n",
            "current evaluation month 12\n",
            "Current average Error per step in episode 0.016003820967269867\n",
            "Current average Error per step in episode 0.016108461431334212\n",
            "Average Error per step in episode 0.016108461431334212\n",
            "Epsidoe Time: 1829.4899 seconds\n",
            "current evaluation month 2\n",
            "step = 29372: AverageReturn = -28151.146484, AverageEpisodeLength = 17476.000000\n",
            "New best eval, saving policy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current collect month 11\n",
            "Current average Error per step in episode 0.016248329229179816\n",
            "Current average Error per step in episode 0.016867127968737516\n",
            "Average Error per step in episode 0.016867127968737516\n",
            "Epsidoe Time: 465.7970 seconds\n",
            "step = 32252: loss = -39.896386990282274\n",
            "average_actor_loss = 1.5269437962108188 : average_critic_loss = 0.17417976821565795 : average_alpha_loss=-41.59751057094998\n",
            "\n",
            "current collect month 1\n",
            "Current average Error per step in episode 0.016900785450470256\n",
            "Current average Error per step in episode 0.017224076341339812\n",
            "Average Error per step in episode 0.017224076341339812\n",
            "Epsidoe Time: 482.7302 seconds\n",
            "step = 35228: loss = -43.843683447889106\n",
            "average_actor_loss = 1.538834303417193 : average_critic_loss = 0.17316086023985858 : average_alpha_loss=-45.55567862782427\n",
            "\n",
            "eval episode at step 35228\n",
            "Current average Error per step in episode 0.01563818970533033\n",
            "Current average Error per step in episode 0.01594436737098569\n",
            "current evaluation month 4\n",
            "Current average Error per step in episode 0.015421546070364722\n",
            "Current average Error per step in episode 0.015353379782388496\n",
            "current evaluation month 6\n",
            "Current average Error per step in episode 0.015183034645419835\n",
            "Current average Error per step in episode 0.015172662967870775\n",
            "current evaluation month 8\n",
            "Current average Error per step in episode 0.015224545784116347\n",
            "Current average Error per step in episode 0.015208328364505657\n",
            "current evaluation month 10\n",
            "Current average Error per step in episode 0.015267841282798764\n",
            "Current average Error per step in episode 0.015255094376103642\n",
            "current evaluation month 12\n",
            "Current average Error per step in episode 0.01527156845390756\n",
            "Current average Error per step in episode 0.01529464034081456\n",
            "Average Error per step in episode 0.01529464034081456\n",
            "Epsidoe Time: 1819.7549 seconds\n",
            "current evaluation month 2\n",
            "step = 35228: AverageReturn = -26728.914062, AverageEpisodeLength = 17476.000000\n",
            "New best eval, saving policy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current collect month 3\n",
            "Current average Error per step in episode 0.015501993424134683\n",
            "Current average Error per step in episode 0.015283838811229713\n",
            "Average Error per step in episode 0.015283838811229713\n",
            "Epsidoe Time: 487.1781 seconds\n",
            "step = 38200: loss = -47.86870759139953\n",
            "average_actor_loss = 1.5345785361921482 : average_critic_loss = 0.17274483862868228 : average_alpha_loss=-49.576030951971\n",
            "\n",
            "current collect month 11\n",
            "Current average Error per step in episode 0.015459164350278814\n",
            "Current average Error per step in episode 0.015917063075753447\n",
            "Average Error per step in episode 0.015917063075753447\n",
            "Epsidoe Time: 469.5627 seconds\n",
            "step = 41080: loss = -51.84449823697408\n",
            "average_actor_loss = 1.51803221884701 : average_critic_loss = 0.16896489304490386 : average_alpha_loss=-53.53149532874425\n",
            "\n",
            "eval episode at step 41080\n",
            "Current average Error per step in episode 0.015770364008317814\n",
            "Current average Error per step in episode 0.015929244880752622\n",
            "current evaluation month 4\n",
            "Current average Error per step in episode 0.015429771458931726\n",
            "Current average Error per step in episode 0.015368798692786656\n",
            "current evaluation month 6\n",
            "Current average Error per step in episode 0.01519124171585138\n",
            "Current average Error per step in episode 0.015162671242485371\n",
            "current evaluation month 8\n",
            "Current average Error per step in episode 0.015212847456746887\n",
            "Current average Error per step in episode 0.015192812895919748\n",
            "current evaluation month 10\n",
            "Current average Error per step in episode 0.015260012476795358\n",
            "Current average Error per step in episode 0.015243805797522969\n",
            "current evaluation month 12\n",
            "Current average Error per step in episode 0.01525745024064668\n",
            "Current average Error per step in episode 0.01528436210348399\n",
            "Average Error per step in episode 0.01528436210348399\n",
            "Epsidoe Time: 1823.2471 seconds\n",
            "current evaluation month 2\n",
            "step = 41080: AverageReturn = -26710.951172, AverageEpisodeLength = 17476.000000\n",
            "New best eval, saving policy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current collect month 11\n",
            "Current average Error per step in episode 0.015468797732203745\n",
            "Current average Error per step in episode 0.01592029048987324\n",
            "Average Error per step in episode 0.01592029048987324\n",
            "Epsidoe Time: 467.7783 seconds\n",
            "step = 43960: loss = -55.73919494549433\n",
            "average_actor_loss = 1.5185020974526802 : average_critic_loss = 0.16707836002784057 : average_alpha_loss=-57.42477538055844\n",
            "\n",
            "current collect month 1\n",
            "Current average Error per step in episode 0.015182457257708799\n",
            "Current average Error per step in episode 0.016129400074687966\n",
            "Average Error per step in episode 0.016129400074687966\n",
            "Epsidoe Time: 492.3202 seconds\n",
            "step = 46936: loss = -59.700504111987286\n",
            "average_actor_loss = 1.5164878581880883 : average_critic_loss = 0.1659513820283195 : average_alpha_loss=-61.38294338923629\n",
            "\n",
            "eval episode at step 46936\n",
            "Current average Error per step in episode 0.01573907779086206\n",
            "Current average Error per step in episode 0.01581652862997044\n",
            "current evaluation month 4\n",
            "Current average Error per step in episode 0.015493916269004824\n",
            "Current average Error per step in episode 0.01535681129040096\n",
            "current evaluation month 6\n",
            "Current average Error per step in episode 0.015212137972458994\n",
            "Current average Error per step in episode 0.015206454134879553\n",
            "current evaluation month 8\n",
            "Current average Error per step in episode 0.015242826025937288\n",
            "Current average Error per step in episode 0.01521484305793865\n",
            "current evaluation month 10\n",
            "Current average Error per step in episode 0.015266741591174268\n",
            "Current average Error per step in episode 0.015283625466482957\n",
            "current evaluation month 12\n",
            "Current average Error per step in episode 0.015319315949030464\n",
            "Current average Error per step in episode 0.0153530570130119\n",
            "Average Error per step in episode 0.0153530570130119\n",
            "Epsidoe Time: 1826.9293 seconds\n",
            "current evaluation month 2\n",
            "step = 46936: AverageReturn = -26831.001953, AverageEpisodeLength = 17476.000000\n",
            "current collect month 11\n",
            "Current average Error per step in episode 0.015552388873719986\n",
            "Current average Error per step in episode 0.0159753430619954\n",
            "Average Error per step in episode 0.0159753430619954\n",
            "Epsidoe Time: 470.7322 seconds\n",
            "step = 49816: loss = -63.64770684242249\n",
            "average_actor_loss = 1.5284306037757132 : average_critic_loss = 0.1649739036646982 : average_alpha_loss=-65.34111136727863\n",
            "\n",
            "current collect month 7\n",
            "Current average Error per step in episode 0.015128293402275558\n",
            "Current average Error per step in episode 0.015025863742968028\n",
            "Average Error per step in episode 0.015025863742968028\n",
            "Epsidoe Time: 487.3725 seconds\n",
            "step = 52792: loss = -67.60930380513591\n",
            "average_actor_loss = 1.527061337665204 : average_critic_loss = 0.1629143802484157 : average_alpha_loss=-69.2992794488066\n",
            "\n",
            "eval episode at step 52792\n",
            "Current average Error per step in episode 0.015520126771408163\n",
            "Current average Error per step in episode 0.015743509766895005\n",
            "current evaluation month 4\n",
            "Current average Error per step in episode 0.015342348750108214\n",
            "Current average Error per step in episode 0.015224652694082736\n",
            "current evaluation month 6\n",
            "Current average Error per step in episode 0.015061820861421986\n",
            "Current average Error per step in episode 0.015020802246836175\n",
            "current evaluation month 8\n",
            "Current average Error per step in episode 0.015053787536275683\n",
            "Current average Error per step in episode 0.01503068062343761\n",
            "current evaluation month 10\n",
            "Current average Error per step in episode 0.015096072749365254\n",
            "Current average Error per step in episode 0.015113892261030586\n",
            "current evaluation month 12\n",
            "Current average Error per step in episode 0.015153014650614584\n",
            "Current average Error per step in episode 0.015204885547037028\n",
            "Average Error per step in episode 0.015204885547037028\n",
            "Epsidoe Time: 1829.8068 seconds\n",
            "current evaluation month 2\n",
            "step = 52792: AverageReturn = -26572.058594, AverageEpisodeLength = 17476.000000\n",
            "New best eval, saving policy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
            "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current collect month 7\n",
            "Current average Error per step in episode 0.015044726497177507\n",
            "Current average Error per step in episode 0.015113405275305932\n",
            "Average Error per step in episode 0.015113405275305932\n",
            "Epsidoe Time: 487.9825 seconds\n",
            "step = 55768: loss = -71.69826599346695\n",
            "average_actor_loss = 1.4594991533185846 : average_critic_loss = 0.16018748543505626 : average_alpha_loss=-73.31795262521312\n",
            "\n",
            "current collect month 9\n",
            "Current average Error per step in episode 0.014466642940209959\n",
            "Current average Error per step in episode 0.014599404985173823\n",
            "Average Error per step in episode 0.014599404985173823\n",
            "Epsidoe Time: 471.2318 seconds\n",
            "step = 58648: loss = -75.59362738132477\n",
            "average_actor_loss = 1.5118344732042817 : average_critic_loss = 0.15828100815674084 : average_alpha_loss=-77.2637429051929\n",
            "\n",
            "eval episode at step 58648\n",
            "Current average Error per step in episode 0.01540064903768313\n",
            "could not calculate power flow\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d6836186eabf>\u001b[0m in \u001b[0;36m<cell line: 284>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain_episode_counter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrain_episodes_per_eval_episode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"eval episode at step {step}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m       \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_eval_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m       \u001b[0mlog_eval_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m       \u001b[0mtrain_episode_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-d6836186eabf>\u001b[0m in \u001b[0;36mget_eval_metrics\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;31m##########################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_eval_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m   \u001b[0meval_actor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_actor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/train/actor.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     self._time_step, self._policy_state = self._driver.run(\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_time_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/drivers/py_driver.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m       \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m       \u001b[0mnext_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0;31m# When using observer (for the purpose of training), only the previous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/environments/py_environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    234\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_time_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-d96e1b4b69ea>\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0mcurrent_vm_pu_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateAndRunNetSimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_casted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mcurrent_vm_pu_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_vm_pu_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'astype'"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#This file is mainly copied from Tensorflow SAC Minitaur example\n",
        "#https://www.tensorflow.org/agents/tutorials/7_SAC_minitaur_tutorial\n",
        "\n",
        "tempdir = \"/content/tempdir\"\n",
        "\n",
        "eval_save_dir = \"/content/tempdir/eval\"\n",
        "\n",
        "#if os.path.exists(eval_save_dir):\n",
        "#    shutil.rmtree(eval_save_dir)\n",
        "\n",
        "\n",
        "##########################################################\n",
        "# Hyperparamertes\n",
        "##########################################################\n",
        "\n",
        "\n",
        "num_iterations = 1500000 # @param {type:\"integer\"} number of train steps\n",
        "\n",
        "initial_collect_steps = 50000 # @param {type:\"integer\"} number of random steps in the beginning\n",
        "replay_buffer_capacity = 1000000 # @param {type:\"integer\"}\n",
        "#smaller replay buffer capacity than num_iterations can lead to big mistakes in later part of the training if there are no more bad experiences in buffer anymore. In this training it is partly avoided by deleting old replays uniformly and not by fifo\n",
        "\n",
        "batch_size = 1000 # @param {type:\"integer\"} #number of fetched steps from replay buffer per training dataset. Can be varied quiet a bit but smaller batch_sizes lead to longer training time\n",
        "\n",
        "\n",
        "critic_learning_rate = 3e-4 # @param {type:\"number\"} learning rate for critic NN\n",
        "actor_learning_rate = 3e-4 # @param {type:\"number\"} learning rate for actor NN\n",
        "alpha_learning_rate = 3e-4 # @param {type:\"number\"} learning rate for alpha factor which regulates entropy which is the regulating factor for exploration/exploitation tradeoff in SAC\n",
        "#more on SAC here https://spinningup.openai.com/en/latest/algorithms/sac.html\n",
        "\n",
        "#haven't changed these factors from the example\n",
        "target_update_tau = 0.005 # @param {type:\"number\"}\n",
        "target_update_period = 1 # @param {type:\"number\"}\n",
        "gamma = 0.99 # @param {type:\"number\"}\n",
        "\n",
        "reward_scale_factor = 1.0 # @param {type:\"number\"} at some point I set the factor to two but since reward is also scaled in env small changes should not matter to much\n",
        "\n",
        "#fully connected layer describtion of NN\n",
        "actor_fc_layer_params = (100, 100)\n",
        "critic_joint_fc_layer_params = (100, 100)\n",
        "\n",
        "num_eval_episodes = 1 # @param {type:\"integer\"}\n",
        "train_episodes_per_eval_episode = 2 # @param {type:\"integer\"}\n",
        "\n",
        "\n",
        "##########################################################\n",
        "# Environment\n",
        "##########################################################\n",
        "#env = CigrePPEnv(False, \"\", [1], 0)\n",
        "#env.reset()\n",
        "\n",
        "#print('Observation Spec:')\n",
        "#print(env.time_step_spec().observation)\n",
        "#print('Action Spec:')\n",
        "#print(env.action_spec())\n",
        "\n",
        "#utils.validate_py_environment(env, episodes=2)\n",
        "\n",
        "\n",
        "\n",
        "collect_env = CigrePPEnv(False, \"collect_env_log.db\",[1,3,5,7,9,11], 1, \"collect\")\n",
        "eval_env = CigrePPEnv(True, \"eval_env_log.db\", [2,4,6,8,10,12], 0, \"evaluation\")\n",
        "\n",
        "\n",
        "\n",
        "##########################################################\n",
        "# Distribution Strategy\n",
        "##########################################################\n",
        "#strategy describes the hardware usesage which depends on the training platform\n",
        "use_gpu = False\n",
        "\n",
        "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)\n",
        "\n",
        "\n",
        "\n",
        "##########################################################\n",
        "# Agent\n",
        "##########################################################\n",
        "observation_spec, action_spec, time_step_spec = (\n",
        "      spec_utils.get_tensor_specs(collect_env))\n",
        "\n",
        "\n",
        "with strategy.scope():\n",
        "  critic_net = critic_network.CriticNetwork(\n",
        "        (observation_spec, action_spec),\n",
        "        observation_fc_layer_params=None,\n",
        "        action_fc_layer_params=None,\n",
        "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
        "        kernel_initializer='glorot_uniform',\n",
        "        last_kernel_initializer='glorot_uniform')\n",
        "\n",
        "\n",
        "#used ActorNetwork instead of ActorDistributionNetwork to get deterministic results\n",
        "#set activation layer to None if negative value should be trained although input/output normalization is advised\n",
        "with strategy.scope():\n",
        "    actor_net = actor_network.ActorNetwork(\n",
        "      observation_spec,\n",
        "      action_spec,\n",
        "      fc_layer_params=actor_fc_layer_params,\n",
        "      activation_fn=tf.keras.activations.relu)\n",
        "\n",
        "\n",
        "with strategy.scope():\n",
        "  train_step = train_utils.create_train_step()\n",
        "\n",
        "  tf_agent = sac_agent.SacAgent(\n",
        "        time_step_spec,\n",
        "        action_spec,\n",
        "        actor_network=actor_net,\n",
        "        critic_network=critic_net,\n",
        "        actor_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=actor_learning_rate),\n",
        "        critic_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=critic_learning_rate),\n",
        "        alpha_optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=alpha_learning_rate),\n",
        "        target_update_tau=target_update_tau,\n",
        "        target_update_period=target_update_period,\n",
        "        td_errors_loss_fn=tf.math.squared_difference,\n",
        "        gamma=gamma,\n",
        "        reward_scale_factor=reward_scale_factor,\n",
        "        train_step_counter=train_step)\n",
        "\n",
        "  tf_agent.initialize()\n",
        "\n",
        "\n",
        "\n",
        "##########################################################\n",
        "# Replay Buffer\n",
        "##########################################################\n",
        "#not really changed these variables from example, test trainings of different values 2-100 seemed not to make a big difference\n",
        "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=3.0, min_size_to_sample=3, error_buffer=3.0)\n",
        "\n",
        "#mainly as in example, just changed remover to Uniform to avoid bad results if the buffer starts deleting the first experiences\n",
        "table_name = 'uniform_table'\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_capacity,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Uniform(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    tf_agent.collect_data_spec,\n",
        "    sequence_length=2,\n",
        "    table_name=table_name,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "\n",
        "#preftch(tf.data.AUTOTUNE) did not seem to make a runtime improvement\n",
        "dataset = reverb_replay.as_dataset(\n",
        "      sample_batch_size=batch_size, num_steps=2).prefetch(1)\n",
        "experience_dataset_fn = lambda: dataset\n",
        "\n",
        "\n",
        "\n",
        "##########################################################\n",
        "# Policies\n",
        "##########################################################\n",
        "#eager polcies just accelerate the training\n",
        "tf_eval_policy = tf_agent.policy\n",
        "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_eval_policy, use_tf_function=True)\n",
        "\n",
        "tf_collect_policy = tf_agent.collect_policy\n",
        "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_collect_policy, use_tf_function=True)\n",
        "\n",
        "\n",
        "random_policy = random_py_policy.RandomPyPolicy(\n",
        "  collect_env.time_step_spec(), collect_env.action_spec())\n",
        "\n",
        "\n",
        "\n",
        "##########################################################\n",
        "# Actors\n",
        "##########################################################\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  reverb_replay.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2,\n",
        "  stride_length=1)\n",
        "\n",
        "\n",
        "initial_collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  random_policy,\n",
        "  train_step,\n",
        "  steps_per_run=initial_collect_steps,\n",
        "  observers=[rb_observer])\n",
        "\n",
        "print(f\"Initial Collect Actor\")\n",
        "initial_collect_actor.run()\n",
        "print(f\"Initial Collect Actor finished\")\n",
        "\n",
        "#smaller collect metric, no summary_dir leading to no collect metric both did not make a significant runtime improvement\n",
        "env_step_metric = py_metrics.EnvironmentSteps()\n",
        "collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  collect_policy,\n",
        "  train_step,\n",
        "  steps_per_run=1,\n",
        "  metrics=actor.collect_metrics(10),\n",
        "  #summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
        "  observers=[rb_observer, env_step_metric])\n",
        "\n",
        "\n",
        "eval_actor = actor.Actor(\n",
        "  eval_env,\n",
        "  eval_policy,\n",
        "  train_step,\n",
        "  episodes_per_run=num_eval_episodes,\n",
        "  metrics=actor.eval_metrics(num_eval_episodes),\n",
        "  summary_dir=eval_save_dir,\n",
        ")\n",
        "\n",
        "saver = PolicySaver(tf_agent.policy)\n",
        "\n",
        "\n",
        "##########################################################\n",
        "# Learners\n",
        "##########################################################\n",
        "\n",
        "agent_learner = learner.Learner(\n",
        "  tempdir,\n",
        "  train_step,\n",
        "  tf_agent,\n",
        "  experience_dataset_fn,\n",
        "  strategy=strategy)\n",
        "\n",
        "\n",
        "##########################################################\n",
        "# Metrics and Evaluation\n",
        "##########################################################\n",
        "def get_eval_metrics():\n",
        "  eval_actor.run()\n",
        "  results = {}\n",
        "  for metric in eval_actor.metrics:\n",
        "    results[metric.name] = metric.result()\n",
        "  return results\n",
        "\n",
        "#metrics = get_eval_metrics()\n",
        "\n",
        "#logs AverageReturn as sum of rewards per Episode\n",
        "def log_eval_metrics(step, metrics):\n",
        "  eval_results = (', ').join(\n",
        "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
        "  print('step = {0}: {1}'.format(step, eval_results))\n",
        "  with open(\"training_log.txt\", \"a\") as logfile:\n",
        "    logfile.write('step = {0}: {1}\\n'.format(step, eval_results))\n",
        "\n",
        "#log_eval_metrics(0, metrics)\n",
        "\n",
        "\n",
        "##########################################################\n",
        "# Training\n",
        "##########################################################\n",
        "#try:\n",
        "#  %%time\n",
        "#except:\n",
        "#  pass\n",
        "\n",
        "# Reset the train step\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
        "max_return = avg_return\n",
        "print(f\"Initial max return set to {max_return}\")\n",
        "\n",
        "\n",
        "print(f\"Training\")\n",
        "\n",
        "train_episode_counter = 0\n",
        "\n",
        "average_total_loss = 0\n",
        "average_actor_loss = 0\n",
        "average_critic_loss = 0\n",
        "average_alpha_loss = 0\n",
        "episode_step_counter = 0\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  # Training, collect one step, train one step from replay buffer\n",
        "  collect_actor.run()\n",
        "  loss_info = agent_learner.run(iterations=1)\n",
        "  episode_step_counter += 1\n",
        "\n",
        "  average_total_loss += loss_info.loss.numpy()\n",
        "  average_actor_loss += loss_info.extra.actor_loss.numpy()\n",
        "  average_critic_loss += loss_info.extra.critic_loss.numpy()\n",
        "  average_alpha_loss += loss_info.extra.alpha_loss.numpy()\n",
        "\n",
        "  #print(f\"actor_loss: {loss_info.extra.actor_loss.numpy()} ; average_critic_loss:{loss_info.extra.critic_loss.numpy()} ; alpha_loss:{loss_info.extra.alpha_loss.numpy()}\")\n",
        "\n",
        "  #print(loss_info)\n",
        "\n",
        "  # Evaluating.\n",
        "  step = agent_learner.train_step_numpy\n",
        "\n",
        "  if collect_env.is_episode_finished():\n",
        "    train_episode_counter += 1\n",
        "\n",
        "    average_total_loss /= episode_step_counter\n",
        "    average_actor_loss /= episode_step_counter\n",
        "    average_critic_loss /= episode_step_counter\n",
        "    average_alpha_loss /= episode_step_counter\n",
        "\n",
        "\n",
        "    print('step = {0}: loss = {1}'.format(step, average_total_loss))\n",
        "    print('average_actor_loss = {0} : average_critic_loss = {1} : average_alpha_loss={2}\\n'.format(average_actor_loss, average_critic_loss, average_alpha_loss))\n",
        "    with open(\"training_log.txt\", \"a\") as logfile:\n",
        "      logfile.write('step = {0}: loss = {1}\\n'.format(step, average_total_loss))\n",
        "      logfile.write('average_actor_loss = {0} : average_critic_loss = {1} : average_alpha_loss={2}\\n'.format(average_actor_loss, average_critic_loss, average_alpha_loss))\n",
        "\n",
        "    average_total_loss = 0\n",
        "    average_actor_loss = 0\n",
        "    average_critic_loss = 0\n",
        "    average_alpha_loss = 0\n",
        "    episode_step_counter = 0\n",
        "\n",
        "    if train_episode_counter == train_episodes_per_eval_episode:\n",
        "      print(f\"eval episode at step {step}\")\n",
        "      metrics = get_eval_metrics()\n",
        "      log_eval_metrics(step, metrics)\n",
        "      train_episode_counter = 0\n",
        "\n",
        "      #export best actor network, prevents overfitting\n",
        "      if (metrics[\"AverageReturn\"] > max_return):\n",
        "        print(f\"New best eval, saving policy\")\n",
        "        max_return = metrics[\"AverageReturn\"]\n",
        "        saver.save('actor_policy')\n",
        "        #saver.save_checkpoint('actor_policy_checkpoint')\n",
        "\n",
        "\n",
        "rb_observer.close()\n",
        "reverb_server.stop()\n",
        "\n",
        "#shutil.rmtree(eval_save_dir)\n",
        "#shutil.rmtree(tempdir)\n"
      ]
    }
  ]
}